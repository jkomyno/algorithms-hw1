
\section{Algoritmi}
\label{cap:algorithms}

\subsection{Prim con Binary Heap}

La versione vista a lezione di Prim prevede i seguenti step:

\begin{enumerate}
    \item Creare una lista $\pi$ non inizializzata di dimensione pari a quella dei vertici del grafo;
    \item Per ogni vertice, inizializzare le chiavi a $+\infty$;
    \item Assegnare il valore $0$ ad una chiave arbitraria (radice dell'MST);
    \item Creare una coda di priorità basata su Min Heap che contenga tutti i nodi non ancora inseriti nell'MST;
    \item Fino a che la coda non è vuota, estrarre il nodo $u$ avente chiave minore dalla coda;
    \item Iterare lungo i vertici $v$ adiacenti al nodo $u$ dentro al ciclo $while$;
    \item Se $v$ appartiene alla coda e il peso dell'arco $(u, v)$ è $<$ della chiave di $v$, inserire l'arco $(u, v)$ allo Spanning Tree $\pi$ e decrementare il valore della chiave di $v$ al peso dell'arco $(u, v)$;
    \item Restituire $\pi$.
\end{enumerate}

\noindent Il listato \ref{listing:prim-binary-heap} contiene la nostra implementazione dell'algoritmo, step per step. La coda di priorità usata si basa su una struttura dati di tipo Min Binary Heap. \\

\begin{listing}[!ht]
\begin{minted}{c++}
// PrimBinaryHeap/prim_binary_heap_mst.h

// step 1
auto vertexes = adj_map_graph.get_vertexes();
const size_t n_stop = vertexes.size();
std::vector<Edge<Label, Weight>> mst(n_stop);

// step 2
constexpr Weight Infinity = std::numeric_limits<Weight>::max();
std::vector<Weight> keys(vertexes.size(), Infinity);

// step 3
keys.at(0) = Weight(0);

// step 4
constexpr bool IsAlreadyHeap = true;
auto min_pq(priority_queue::make_min_priority_queue<IsAlreadyHeap>(
    std::move(keys), std::move(vertexes)));

while (!(min_pq.empty() && n_stop == mst.size())) {
  // step 5
  auto u = min_pq.top();
  min_pq.pop();

  // step 6
  for (const auto [v, weight] : adj_map_graph.adjacent_vertexes(u)) {
    // step 7
    if (min_pq.contains(v) && weight < min_pq.key_at(v)) {
      min_pq.update_key(weight, v);
      mst.at(v) = Edge<Label, Weight>(u, v, weight);
    }
  }
}

// step 8
return mst;
\end{minted}
\caption{Implementazione di PrimBinaryHeap. I commenti del file originale sono stati omessi per una maggiore compattezza.}
\label{listing:prim-binary-heap}
\end{listing}

\subsubsection{Osservazioni}

\begin{itemize}
    \item Abbiamo utilizzato \mintinline{c++}{std::numeric_limits<Weight>::max()} per inizializzare le chiavi al valore massimo assumibile dal tipo generico \mintinline{c++}{Weight}, che è istanziato come \mintinline{c++}{long}. Il massimo valore rappresentabile è quindi \mintinline{c++}{2147483647L}. \\

    \item Poiché scegliamo sempre il primo nodo per inizializzare a 0 la chiave dello step 3, il container \textit{keys} rispetta già la proprietà di Min Heap. La coda di priorità è quindi creata in tempo \complexityConstant{}. \\

    \item Se l'MST che stiamo costruendo arriva a contenere $n - 1$ archi prima che la coda sia svuotata, usciamo prematuramente dal ciclo. \\

    \item Determinare se la coda contiene un nodo $v$ e se il peso $weight$ corrente è minore della chiave associata a $v$ ha complessità temporale \complexityConstant{} ammortizzata.\\

    \item Aggiornare il valore della chiave associata ad un nodo $v$ ha complessità temporale pari a \complexityLogN{}.\\
\end{itemize}


\subsection{Kruskal Naive}

La versione vista a lezione di Kruskal Naive prevede i seguenti step:

\begin{enumerate}
    \item Inizializzare l'oggetto da restituire in output $A$ (che conterrà quindi l'MST) come insieme vuoto;
    \item Ordinare gli archi del grafo per costo non decrescente;
    \item Iterare lungo gli archi in tale ordine;
    \item Aggiungere $edge$ ad $A$, ma solo se $A \cup \{ edge \}$ è aciclico;
    \item Restituire $A$.
\end{enumerate}

\noindent Il listato \ref{listing:kruskal-naive} contiene la nostra implementazione dell'algoritmo, step per step.

\begin{listing}[!ht]
\begin{minted}{c++}
// KruskalNaive/kruskal_naive_mst.h

// step 1
const size_t n = adj_map_graph.vertexes_size();
AdjacencyMapGraph<Label, Weight> mst_set_graph({}, n);

// step 2
auto edges = adj_map_graph.get_sorted_edges(std::less<>{});

DFSCycleDetection<Label, Weight> dfs(&mst_set_graph);

// step 3
for (auto it = edges.cbegin();
     !(it == edges.cend() && n == mst_set_graph.vertexes_size()); ++it) {
  const auto &edge = *it;

  // step 4
  if (!dfs.are_connected(edge.from, edge.to)) {
    mst_set_graph.add_edge(edge);
  }
}

// step 5
return mst_set_graph.get_edges();
\end{minted}
\caption{Implementazione di KruskalNaive. I commenti del file originale sono stati omessi per una maggiore compattezza.}
\label{listing:kruskal-naive}
\end{listing}

\noindent L'algoritmo Kruskal Naive è stato implementato a partire dallo pseudo codice visto in classe. Per individuare se la foresta $A \cup \{ e \}$ dello step 4 dell'algoritmo sia aciclica, abbiamo usato \textbf{Depth First Search} (DFS). \\

\noindent Ci sono venute in mente 2 idee di utilizzo di \textit{DFS} (definite nel file \textit{Shared/DFSCycleDetection.h}) che affrontano il problema da 2 punti di vista diversi.

\paragraph{Idea 1: Ricerca di un ciclo}\mbox{} \\

\noindent L'idea consiste nell'aggiungere un arco alla foresta $A$, controllare se l'inserimento ha causato un ciclo, e eventualmente rimuovere l'arco appena inserito. Questo approccio risponde alla domanda ``Esiste un ciclo all'interno della foresta $A$?'', ed è implementato dal metodo \codeinline{has\_cycle()} della classe \codeinline{DFSCycleDetection}.

\paragraph{Idea 2: Ricerca di un cammino}\mbox{} \\

\noindent Quest'altra idea è meno generale ma più efficiente rispetto alla precedente. Consiste nell'assumere che la foresta $A$ sia rimasta aciclica dopo l'iterazione precedente del ciclo, per poi controllare (stavolta senza effettuare nessun inserimento preliminare dell'arco corrente) che non esista nessun cammino che colleghi i due estremi dell'arco corrente in $A$.
Se tale cammino esistesse già e noi inserissimo l'arco corrente, andremmo ad introdurre un ciclo in $A$, cosa che vogliamo evitare.\\
Per dirla nei termini di DFS, andremmo ad inserire un BACK EDGE. Viceversa, se tale cammino non esiste (ovvero, se DFS etichetta tutti i lati di $A$ come DISCOVERY EDGE), possiamo aggiungere l'arco corrente con la sicurezza di non introdurre alcun ciclo.
Questo approccio risponde alla domanda ``Dato il lato $(u, v)$ e la foresta $A$, esiste un percorso tra $u$ e $v$?'', ed è implementato dal metodo \codeinline{are\_connected(source, target)} della classe \codeinline{DFSCycleDetection}. \\

\vspace{0.5cm}

\noindent L'idea 2 si è rivelata più efficace per i dataset a disposizione. Nel caso di un grafo in input con 100.000 nodi e circa 130.000, questa idea è il ~30\% più efficiente in termini di tempo rispetto all'idea 1. In nessuna delle due implementazione di DFS è stato necessario usare esplicitamente le etichettature DISCOVERY EDGE e BACK EDGE: abbiamo semplicemente usato un container di tipo \codeinline{std::unordered\_set<Label>} per tenere traccia dei nodi visitati.

\subsubsection{Osservazioni}

\begin{itemize}
    \item Abbiamo utilizzato \mintinline{c++}{adj_map_graph.get_sorted_edges(std::less<>{})} per ricevere un \mintinline{c++}{std::vector<Edge<Label, Weight>>} contenente gli archi in ordine non decrescente. Questa operazione costa tempo \complexityMLogM{} e usa il comparatore generico \mintinline{c++}{std::less<>}.\\

    \item Nello pseudo-codice non viene esplicitato in modo concreto come effettuare la verifica della ciclicità, i.e. verificare se $A \cup \{ e \}$ sia aciclico, lasciando così campo libero all'implementazione. Abbiamo deciso di usare DFS come spiegato nel paragrafo \textit{Idea 2: Ricerca di un cammino}. \\

    \item Se l'MST che stiamo costruendo arriva a contenere $n$ vertici prima di aver attraversato tutti gli archi, usciamo prematuramente dal ciclo.\\

    \item L'algoritmo di ricerca di cicli implementato con DFS è in grado di gestire anche i casi particolari di self-loop, come mostrato in figura \ref{fig:SelfLoop} nell'esempio a sinistra.

\begin{figure}[h]
	\caption{Due esempi di grafi ciclici.}
	\centering
	\includegraphics[width=0.7\textwidth]{./images/ExampleSelfLoop.png}
	\label{fig:SelfLoop}
\end{figure}

\begin{listing}[!ht]
\begin{minted}{c++}
// Shared/DFSCycleDetection.h

bool are_connected_helper(const Label &source, const Label &target,
                          std::unordered_set<Label> &visited) const {
  // segna il nodo corrente come visitato
  visited.insert(source);

  // il vertice source sarà il primo ad essere processato
  std::stack<Label> stack;
  stack.push(source);

  while (!stack.empty()) {
    // estraiamo il vertice sorgente corrente dalla pila
    const auto curr_source = stack.top();
    stack.pop();

    // se il vertice sorgente corrente ha un self loop o se è direttamente collegato
    // al nodo target, allora esiste un cammino da source a target
    if (curr_source == target ||
        adj_map_graph_ptr->has_edge(curr_source, target)) {
      return true;
    }

    if (adj_map_graph_ptr->has_vertex(curr_source)) {
      for (const auto &[u, _] : adj_map_graph_ptr->adj_map.at(curr_source)) {
        // inseriamo tutti i nodi adiacenti non incontrati in precedenza nella pila,
        // per essere processati all'iterazione successiva.
        if (!visited.count(u)) {
          visited.insert(u);
          stack.push(u);
        }
      }
    }
  }

  return false;
}
\end{minted}
\caption{Funzione \codeinline{are\_connected\_helper()} per la rilevazione di cammini tra due nodi.}
\label{listing:dfs-cycle}
\end{listing}

    Per quanto riguarda invece l'esempio a destra in figura ~\ref{fig:SelfLoop}, viene automaticamente gestito dalle scelta implementativa fatta per la rappresentazione interna di un grafo. Infatti tale esempio, preso come input verrebbe trasformato in un grafo in cui sussiste un solo arco tra il nodo 0 e 1. L'arco che verrebbe preso in considerazione sarebbe quello di peso più basso, mentre l'altro arco viene scartato automaticamente. Pertanto all'interno della nostra struttura dati per rappresentare i grafi non sarà mai possibile avere nodi collegati in questo modo.\\

    \item Un'altra ulteriore ottimizzazione di tale algoritmo è la verifica ad ogni aggiunta di un nuovo arco che non crea un ciclo nel grafo, della soglia massima di archi che un MST può avere, che come visto in classe è pari n - 1. Pertanto non appena l'MST che si sta calcolando raggiunge la soglia prestabilita e possibile ritornare immediatamente l'MST.

\end{itemize}


\subsection{Kruskal con Disjoint Set}

La versione vista a lezione di Kruskal con Disjoint-Set (Union-Find) prevede i seguenti step:

\begin{enumerate}
    \item Inizializzare l'oggetto da restituire in output $A$ (che conterrà quindi l'MST) come insieme vuoto;
    \item Inizializzare una struttura dati Disjoint-Set con la lista di vertici del grafo;
    \item Ordinare gli archi del grafo per costo non decrescente;
    \item Iterare lungo gli archi $e = (v, w)$ in tale ordine;
    \item Se $v$ e $w$ non sono connessi, aggiungere il loro arco ad $A$;
    \item Aggiornare la struttura dati Disjoint-Set unendo $v$ e $w$;
    \item Restituire $A$.
\end{enumerate}

\noindent Il listato \ref{listing:kruskal-union-find} contiene la nostra implementazione dell'algoritmo, step per step.

\begin{listing}[!ht]
\begin{minted}{c++}
// KruskalUnionFind/kruskal_mst.h

// step 1
std::vector<Edge<Label, Weight>> mst;
const size_t n_stop = adj_map_graph.vertexes_size() - 1;
mst.reserve(n_stop);

// step 2
auto vertexes = adj_map_graph.get_vertexes();
disjoint_set::DisjointSet<Label> disjoint_set(std::move(vertexes));

// step 3
const auto edges = adj_map_graph.get_sorted_edges(std::less<>{});

// step 4
for (auto it = edges.cbegin(); !(it == edges.cend() && n_stop == mst.size());
     ++it) {
  const auto &edge = *it;
  const auto &[v, w, _] = edge;

  if (!disjoint_set.are_connected(v, w)) {
    // step 5
    mst.push_back(edge);

    // step 6
    disjoint_set.unite(v, w);
  }
}

// step 7
return mst;
\end{minted}
\caption{Implementazione di KruskalUnionFind. I commenti del file originale sono stati omessi per una maggiore compattezza.}
\label{listing:kruskal-union-find}
\end{listing}

\subsubsection{Osservazioni}
\begin{itemize}
    \item Come osservato per KruskalNaive, abbiamo utilizzato il metodo  \mintinline{c++}{get_sorted_edges} \mintinline{c++}{(std::less<>{})} per ricevere un \mintinline{c++}{std::vector<Edge<Label, Weight>>} contenente gli archi in ordine non decrescente. \\

    \item Se l'MST che stiamo costruendo arriva a contenere $n - 1$ archi prima di aver attraversato tutti gli archi del grafo originale, usciamo prematuramente dal ciclo $for$. \\

\end{itemize}


% L'algoritmo di Kruskal con Disjoint Set implementato risulta essere in buona parte fedele a quello già visto in classe, con al differenze che è stato anche ottimizzata la condizione di terminazione, che prevede che al raggiungimento di n-1 archi all'interno dell'mst costruito, l'algoritmo termina restituendo l'mst calcolato fino a quel momento. Per implementare completamente questo algoritmo efficiente di Kruskal è necessario anche la creazione di un Disjoint Set: per fare ciò si e definita una classe astratta di base chiamata "DisjointSetBase" per poi implementarla con la classe "DisjointSet" e "DisjointCompressed" che non fanno altro che implementare i metodi astratti find e unite.
